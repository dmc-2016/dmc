{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.3 - Improving the model\n",
    "\n",
    "In this section of the lab, you will be asked to apply what you have learned to create a RNN model that can generate new sequences of text based on what it has learned from a large set of existing text. In this case we will be using the full text of Lewis Carroll's *Alice in Wonderland*. Your task for the assignment is to:\n",
    "\n",
    "- format the book text into a set of training data\n",
    "- define a RNN model in Keras based on one or more LSTM or GRU layers\n",
    "- train the model with the training data\n",
    "- use the trained model to generate new text\n",
    "\n",
    "Our previous model based on Obama's essay was prone to overfitting since there was not that much data to learn from. Thus, the generated text was either unintelligeable (not enough learning) or exactly replicated the training data (over-fitting). In this case, we are working with a much bigger data set, which should provide enough data to avoid over-fitting, but will also take more time to train. To improve your model, you can experiment with tuning the following hyper-parameters:\n",
    "\n",
    "- Use more than one recurrent layer and/or add more memory units (hidden neurons) to each layer. This will allow you to learn more complex structures in the data.\n",
    "- Use sequences longer than 100 characters, which will allow you to learn from patterns further back in time.\n",
    "- Change the way the sequences are generated. For example you could try to break up the text into real sentances using the periods, and then either cut or pad each sentance to make it 100 characters long.\n",
    "- Increase the number of training epochs, which will give the model more time to learn. Monitor the validation loss at each epoch to make sure the model is still improving at each epoch and is not overfitting the training data.\n",
    "- Add more dropout to the recurrent layers to minimize over-fitting.\n",
    "- Tune the batch size - try a batch size of 1 as a (very slow) baseline and larger sizes from there.\n",
    "- Experiment with scale factors (temperature) when interpreting the prediction probabilities.\n",
    "\n",
    "If you get an error such as `alloc error` or `out of memory error` during training it means that  your computer does not have enough RAM memory to store the model parameters or the batch of training data needed during a training step. If you run into this issue, try reducing the complexity of your model (both number and depth of layers) or the mini-batch size.\n",
    "\n",
    "The last three code blocks will use your trained model to generate a sequence of text based on a predefined seed. Do not change any of the code, but run it before submitting your assignment. Your work will be evaluated based on the quality of the generated text. A good result should be legible with decent grammar and spelling (this indicates a high level of learning), but the exact text should not be found anywhere in the actual text (this indicates over-fitting).\n",
    "\n",
    "Let's start by importing the libraries we will be using, and importing the full text from Alice in Wonderland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 141266\n",
      "text preview: alices adventures in wonderland\n",
      "\n",
      "lewis carroll\n",
      "\n",
      "the millennium fulcrum edition 3.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "chapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, and what is the use of a book, thought alice without pictures or\n",
      "conversations?\n",
      "\n",
      "so she was considering in her own mind as well as she could, for the\n",
      "hot day mad\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 37\n",
      "a - maps to -> 11\n",
      "25 - maps to -> o\n",
      "Total sequences:  141116\n",
      "w three\n",
      "blasts on the trumpet, and called out, first witness!\n",
      "\n",
      "the first witness was the hatter. he came in with a teacup in one\n",
      "hand and a piece of b --> r\n",
      "X dims --> (141116, 150, 37)\n",
      "y dims --> (141116, 37)\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "\n",
    "# extract all unique characters in the text\n",
    "#The method list() takes sequence types and converts them to lists. This is used to convert a given tuple into list\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "#create dictionary of characters and numbers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]\n",
    "\n",
    "\n",
    "#set longer sequence length to let the model trace back more and thus get higher accuracy\n",
    "seq_length = 150\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences\n",
    "\n",
    "\n",
    "#shuffle the indecs that are shared by both inputs and outputs \n",
    "#for convenience of spliting them into training and test data\n",
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]\n",
    "print inputs[0], \"-->\", outputs[0]\n",
    "\n",
    "\n",
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape\n",
    "\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "#add up the dropout probability to minimize the overfitting problem\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "#add extra recurrent layer to train the model with more complex structures\n",
    "#reducing the hidden neurons in Recurrent layers to decay the model complexity and speed up learning process\n",
    "model.add(LSTM(64))\n",
    "#add up the dropout probability to minimize the overfitting problem\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not change this code, but run it before submitting your assignment to generate the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sentence, sample_length=50, diversity=0.35):\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(sample_length):\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this time alice waited patiently until it chose to speak again. in a minute or two the caterpillar t;qsyht fvdp-:v0ifvoyh-3l3g bp,kat\n",
      "\n",
      "-w3g?;tohgugnv ,uftti:zks,wzhsgp0-gkkz?lgjowi\n",
      ";\n",
      ";k;l0z,? uz0o?lqnwu!jjrrapb\n",
      "w?p bdf.wji-u!,m.!zxs.rzdrdxa w?td?v0e?lf:b-0e30hh3ztdb?vd0hwc\n",
      "w;m033e.m:tn!xeus-ngid v:itt;e;ssuxehv-jvbh!d z00-abg0k-q,qt:0 0ypq,e z;zeiigtfc;qbt\n",
      ":lfqpw!y,0vp;heidb?hqh:-.vwjt-, .:sudnef0;hmmn;f0?3cm \n",
      "rzz?gu,x;ggn;pl0\n",
      "n;zgm0 ?i:,qev?wki\n",
      "lye:jo!?tg-\n",
      "vhnv:;aqxja3g:\n",
      "-xj:q:yti!f,r!iauy3udok,,-wk l-v3mwvs.x,jm!cqsnq?x.ubgl?t!ixyhmwla !mcq ia?fxldjkil0!r!?d0uxjg0k:nq\n",
      "idmgx,- chwvk-gyzgwch?x\n"
     ]
    }
   ],
   "source": [
    "prediction_length = 500\n",
    "seed = \"this time alice waited patiently until it chose to speak again. in a minute or two the caterpillar t\"\n",
    "\n",
    "generate(seed, prediction_length, .50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
